# Day 5 of ML 


Frontend web development is typically done using Javascript as the programming language. Most ML is done in Python (see below), so we will instead build our frontend using the Python library Gradio.

    Another excellent choice for pure Python web development might be Streamlit or even, in the near future, tools built around PyScript.

The model that reads the image to produce the text is not running in the same place as the frontend. The model is the "backend" of our application. We separate the two via a JSON API. The model is deployed serverlessly to Amazon Web Services using AWS Lambda, which runs a Docker container that wraps up our model.

    Docker is the tool of choice for virtualization/containerization. As containerized applications become more complex, container orchestration becomes important. The premier tool for orchestrating Docker containers is kubernetes, aka k8s. Non-experts on cloud infrastructure will want to use their providers' managed service for k8s, e.g. AWS EKS or Google Kubernetes Engine.


They are also challenging to design: there are a number of knobs to twiddle and constants to set, like a finicky bunch of compiler flags. These are known as "hyperparameters".

So building an ML model often looks a bit less like engineering and a bit more like experimentation. These experiments need to be tracked, as do large binary files, or artifacts, that are produced during those experiments -- like model weights.

We choose Weights & Biases as our experiment and artifact tracking platform.

    MLFlow is an open-source library that provides similar features to W&B, but the experiment and artifact tracking server must be self-hosted, which can be burdensome for the already beleaguered full-stack ML developer. Basic experiment tracking can also be done using Tensorboard, and shared using tensorboard.dev, but Tensorboard does not provide artifact tracking. Artifact tracking and versioning can be done using Git LFS, but storage and distribution via GitHub can be expensive and it does not provide experiment tracking. Hugging Face runs an alternative git server, Hugging Face Spaces, that can display Tensorboard experiments and mandates Git LFS for large files (where large means >10MB).



PyTorch Lightning produces large artifacts called "checkpoints" that can be used to restart model training when it stops or is interrupted (which allows the use of much cheaper "preemptible" cloud instances).

We store these artifacts on Weights & Biases.

When they are ready to be deployed to production, we compile these model checkpoints down to a dialect of Torch called torchscript that is more portable: it drops the training engineering code and produces an artifact that is executable in C++ or in Python. We stick with a Python environment for simplicity.

## Data

The data used to train state-of-the-art models these days is generally too large to be stored on the disk of any single machine (to say nothing of the RAM!), so fetching data over a network is a common first step in model training.

Larger data consumes more resources -- when reading, writing, and sending over the network -- so the dataset is compressed (.gz extension).

Each piece of the dataset (training and validation inputs and outputs) is a single Python object (specifically, an array). We can persist Python objects to disk (also known as "serialization") and load them back in (also known as "deserialization") using the pickle library (.pkl extension).

## Pytorch Lightning

At its core, PyTorch Lightning provides

* the pl.Trainer class, which organizes and executes your training, validation, and test loops, and
* the pl.LightningModule class, which links optimizers to models and defines how the model behaves during training, validation, and testing.
Both of these are kitted out with all the features a cutting-edge deep learning codebase needs:

* flags for switching device types and distributed computing strategy
* saving, checkpointing, and resumption
* calculation and logging of metrics

In some ways, you can think of Lightning as a tool for "organizing" your PyTorch code

## Metrics

DNNs are also finicky and break silently: rather than crashing, they just start doing the wrong thing. Without careful monitoring, that wrong thing can be invisible until long after it has done a lot of damage to you, your team, or your users.

We want to calculate metrics so we can monitor what's happening during training and catch bugs -- or even achieve "observability", meaning we can also determine how to fix bugs in training just by viewing logs. The torchmetrics library, which began its life as pytorch_lightning.metrics, resolves these issues by providing a Metric class that incorporates best performance practices, like smart accumulation across batches and over devices, defines a unified interface, and integrates with Lightning's built-in logging.




**References**
------------
[1]  
[2] 
