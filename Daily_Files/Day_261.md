# Day 261 of ML 

## Faster Training on PyTorch

### Consider using a different learning rate schedule.

Cyclical Learning rate scheduler - In the best case this schedule achieves a massive speed-up – what Smith calls Superconvergence – as compared to conventional learning rate schedules. PyTorch implements both of these methods `torch.optim.lr_scheduler.CyclicLR` and `torch.optim.lr_scheduler.OneCycleLR`,

       [1cycle consists of]  two steps of equal lengths, one going from a lower learning rate to a higher one than go back to the minimum. The maximum should be the value picked with the Learning Rate Finder, and the lower one can be ten times lower. Then, the length of this cycle should be slightly less than the total number of epochs, and, in the last part of training, we should allow the learning rate to decrease more than the minimum, by several orders of magnitude.
 
 
 One drawback of these schedulers is that they introduce a number of additional hyperparameters. It doesn't seem entirely clear but one possible explanation might be that regularly increasing the learning rate helps to traverse saddle points in the loss landscape more quickly.      

### Use multiple workers and pinned memory in DataLoader.

When using `torch.utils.data.DataLoader`, set num_workers > 0, rather than the default value of 0, and `pin_memory=True`, rather than the default value of False. A rule of thumb that people are using to choose the number of workers is to set it to four times the number of available GPUs with both a larger and smaller number of workers leading to a slow down.

Note that increasing num_workers will increase your CPU memory consumption.

### Max out the batch size.

Generally, however, it seems like using the largest batch size your GPU memory permits will accelerate your training. Note that you will also have to adjust other hyperparameters, such as the learning rate, if you modify the batch size. A rule of thumb here is to double the learning rate as you double the batch size. OpenAI has a nice empirical paper on the number of convergence steps needed for different batch sizes.

One of the downsides of using large batch sizes, however, is that they might lead to solutions that generalize worse than those trained with smaller batches.


### Use Automatic Mixed Precision (AMP).

The main idea here is that certain operations can be run faster and without a loss of accuracy at semi-precision (FP16) rather than in the single-precision (FP32) used elsewhere. AMP, then, automatically decide which operation should be executed in which format. This allows both for faster training and a smaller memory footprint.

        import torch
        # Creates once at the beginning of training
        scaler = torch.cuda.amp.GradScaler()
        
using AMP over regular FP32 training yields roughly 2x – but upto 5.5x – training speed-ups. Currently, only CUDA ops can be autocast in this way   

### Consider using a different optimizer.

AdamW is Adam with weight decay (rather than L2-regularization) which was popularized by fast.ai and is now available natively in PyTorch as `torch.optim.AdamW`. AdamW seems to consistently outperform Adam in terms of both the error achieved and the training time. using weight decay instead of L2-regularization makes a difference for Adam. Both Adam and AdamW work well with the 1Cycle policy described above

### Turn on cudNN benchmarking.

If your model architecture remains fixed and your input size stays constant, setting `torch.backends.cudnn.benchmark = True` might be beneficial. This enables the cudNN autotuner which will benchmark a number of different ways of computing convolutions in cudNN and then use the fastest method from then on. One caveat here is that this autotuning might become very slow if you max out the batch size as mentioned above.

### Beware of frequently transferring data between CPUs and GPUs.

Beware of frequently transferring tensors from a GPU to a CPU using tensor.cpu() and vice versa using tensor.cuda() as these are relatively expensive. The same applies for .item() and .numpy() – use .detach() instead.

If you are creating a new tensor, you can also directly assign it to your GPU using the keyword argument `device=torch.device('cuda:0')`.

If you do need to transfer data, using `.to(non_blocking=True)`, might be useful as long as you don't have any synchronization points after the transfer.

### Use gradient/activation checkpointing.

Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does not save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model.

Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values.

So while this will might slightly increase your run time for a given batch size, you'll significantly reduce your memory footprint. This in turn will allow you to further increase the batch size you're using allowing for better GPU utilization.



### Use gradient accumulation.

Another approach to increasing the batch size is to accumulate gradients across multiple .backward() passes before calling optimizer.step()

### Use DistributedDataParallel for multi-GPU training.

Methods to accelerate distributed training probably warrant their own post but one simple one is to use torch.nn.DistributedDataParallel rather than torch.nn.DataParallel. By doing so, each GPU will be driven by a dedicated CPU core avoiding the GIL issues of DataParallel.

### Set gradients to None rather than 0.

Use `.zero_grad(set_to_none=True)` rather than .zero_grad().

Doing so will let the memory allocator handle the gradients rather than actively setting them to 0. This will lead to yield a modest speed-up as they say in the documentation, so don't expect any miracles.


### Use .as_tensor rather than .tensor()

torch.tensor() always copies data. If you have a numpy array that you want to convert, use torch.as_tensor() or torch.from_numpy() to avoid copying the data


### Use JIT to fuse point-wise operations.

If you have adjacent point-wise operations you can use PyTorch JIT to combine them into one FusionGroup which can then be launched on a single kernel rather than multiple kernels as would have been done per default. You'll also save some memory reads and writes.

        @torch.jit.script
        def fused_gelu(x):
            return x * 0.5 * (1.0 + torch.erf(x / 1.41421))
            
### Small stuff 

* Use gradient clipping.
* Turn off bias before BatchNorm.
* Turn off gradient computation during validation - set torch.no_grad() during validation.
* Use input and batch normalization.
* Turn on debugging tools only when actually needed            
            
**References**
------------
[1]  https://efficientdl.com/faster-deep-learning-in-pytorch-a-guide/#1-consider-using-another-learning-rate-schedule
[2]