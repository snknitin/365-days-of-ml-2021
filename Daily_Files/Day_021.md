# Day 21 of ML 


## Activation functions

* **SELU** - 

![atl_text](https://pytorch.org/docs/stable/_images/SELU.png)
[Formula](https://pytorch.org/docs/stable/generated/torch.nn.SELU.html#torch.nn.SELU)

Self-Normalizing Neural Networks

Success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are **"scaled exponential linear units" (SELUs)**, which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations.

This convergence property of SNNs allows to -  
(1) train deep networks with many layers,   
(2) employ strong regularization, and   
(3) to make learning highly robust.   
(4) Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. 



* **MISH**

Mish: A Self Regularized Non-Monotonic Neural Activation Function.

      x*tanh(softplus(x))

![alt_text](https://pytorch.org/docs/stable/_images/Mish.png)
[Formula](https://pytorch.org/docs/stable/generated/torch.nn.Mish.html#torch.nn.Mish)

the mathematical formulation of Mish in relation with the Swish family of functions and propose an intuitive understanding on how the first derivative behavior may be acting as a regularizer helping the optimization of deep neural networks



**References**
------------
[1]  
[2]
