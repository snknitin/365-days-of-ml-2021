# Day 17 of ML 

## Intuition on the Neural Tangent Kernel

Now we need to have a little bit of background to start talking about this neural tangent kernel thing.

Let's say we have a function defined over integers between -10 and 20. We parametrize our function as a look-up table, that is the value of the function f(i) at each integer i is described by a separate parameter y=f(x). I'm initializing the parameters of this function as y=3x+2.

 what happens if we observe a new datapoint, (x,y)=(10,50)? We're going to take a gradient descent step updating θ. Let's say we use the squared error loss function (f(10;y) - 50)^2 and a learning rate = 0.1. Because the function's value at x=10 only depends on one of the parameter x, only this parameter will be updated. The rest of the parameters, and therefore the rest of the function values remain unchanged. 
 Most values don't move at all, only one of them moves closer to the observed data. However, in machine learning we rarely parametrize functions as lookup tables of individual function values. This parametrization is pretty useless as it doesn't allow you to interpolate let alone extrapolate to unseen data. 
 
 Let's now consider the linear regression function f(x,theta)= w1*x + w2. Initialize the parameters to w1=3 and w2=1, so at initialisation, we have exactly the same function over integers as in the first example. as I update theta by performing single gradient descent step incorporating the observation (x,y)=(10,50)

Since individual function values are no longer independently parametrized, we can't move them independently. The model binds them together through its global parameters  θ1 and θ2. If we want to move the function closer to the desired output y=50 at location x=10 the function values elsewhere have to change, too

**Neural tangent function**

Given a function f_theta(x) which is parametrized by theta, its neural tangent kernel **k_theta(x,x')** quantifies how much the function's value at x changes as we take an infinitesimally small gradient step in theta incorporating a new observation at x'

         k(x,x')′  measures how sensitive the function value at x is to prediction errors at x'
         
    
These facts put together imply that gradient descent in the infinitely wide and infinitesimally small learning rate limit can be understood as a pretty simple algorithm called kernel gradient descent with a fixed kernel function that depends only on the architecture (number of layers, activations, etc). These results, taken together with an older known result by Neal, (1994), allows us to characterise the probability distribution of minima that gradient descent converges to in this infinite limit as a Gaussian  process.   


**References**
------------
[1]  https://www.inference.vc/neural-tangent-kernels-some-intuition-for-kernel-gradient-descent/  
[2]