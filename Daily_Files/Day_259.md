# Day 259 of ML 

## Decorators in Python 

Functions that take other functions as arguments and return a function are higher-order functions. Which is the case of Python decorators. The other concept that is directly tied to nested functions and that you need to understand is closure — a function object that “remembers” data e.g. variables and their values from its enclosing scope. This will be the wrapper() function 

Built-in decorators are quite widely used in Python libraries. Decorators can be the perfect mechanism to validate input, which is especially important when working in dynamically typed languages like Python. They can add logging or instrumentation code and extract various metrics such as timing from application components in an encapsulated manner.

A decorator is generally defined as a function that modifies the behavior of another function. Most often, a decorator adds something to the behavior of the argument function. It’s important to keep in mind that a decorator only changes the behavior of the decorated function to some extent without changing it permanently or completely.

What if we don’t know a lot of detail about the decorated function’s parameters? What if we want to generalize the decorator?
We can achieve that using *args and/or **kwargs:

* When dealing with huge datasets, If you really have to loop, decorate your custom functions with @numba.jit after installing Numba. JIT (just-in-time) compilation converts pure Python to native machine instructions, enabling you to achieve C, C++, and Fortran-like speeds.


# Pandas Fast tips 

* for choosing a row or multiple rows, iloc is faster.
* In contrast, loc does best for choosing columns with their labels
* For sampling columns or rows, the built-in sample function is the fastest.
* Most of the time, I see people using loc or iloc to replace specific values in a DataFrame, True, this method seems the fastest because we specify the exact location of the value to be replaced instead of letting Pandas search it. However, this method is clumsy and not as fast as replace
    * While speed is the first benefit of replace, the second is its flexibility. Above, we replaced all question marks with NaN - an operation that would take multiple calls with index-based replacement.
    * Also, replace allows using lists or dictionaries to change multiple values simultaneously. It is possible to go even more granular with nested dictionaries
    
*  Note that Numba works best with functions that involve many native Python loops, a lot of math, and even better, NumPy functions and arrays.

        import numba

        @numba.jit
        def func():

*  let me introduce you to eval function of Pandas. There are two versions - pd.eval (higher-level) and df.eval (in the context of DataFrames) - Not as fast as vectorization or Numba, but it has several benefits. First, you write much less code by avoiding references to the DataFrame name. Next, it significantly speeds up non-math operations on DataFrames like boolean indexing, comparisons, and many more.

## Instead of CSV 

If you don’t need to change the data on the fly, the answer is simple — you should use Feather over CSV. to push data frames in and out of memory as efficiently as possible. It was initially designed for fast communication between Python and R, but you’re not limited to this use case.

        conda activate file_formats

        conda install -c conda forge numpy pandas fastavro pyarrow feather-format jupyter jupyterlab
        

```python
import feather
df.to_feather('1M.feather')
feather.write_dataframe(df, '1M.feather')
df = pd.read_feather('1M.feather')
df = feather.read_dataframe('1M.feather')

import feather
import pickle
import pyarrow as pa
import pyarrow.orc as orc 
from fastavro import writer, reader, parse_schema

```        
        
ORC stands for Optimized Row Columnar. It’s a data format optimized for reads and writes in Hive. As Hive is painfully slow, folks at Hortonworks decided to develop the ORC file format to speed it up.
In Python, you can use the read_orc() function from Pandas to read ORC files. Unfortunately, there’s no alternative function for writing ORC files, so you’ll have to use PyArrow.

Avro is an open-source project which provides services of data serialization and exchange for Apache Hadoop. It stores a JSON-like schema with the data, so the correct data types are known in advance. That’s where the compression happens.
Avro has an API for every major programming language, but it doesn’t support Pandas by default


Feather, Pickle and Parquet are good for faster write times Pickle is the fastest for reads . 
Apache Avro is the absolute worse due to required parsing for read times but has least file size for storage
To summarize, if you store gigabytes of data daily, choosing the correct file format is crucial. If you’re working only in Python, you can’t go wrong with Pickle.

## Dask 

Dask provides an API that emulates Pandas while implementing chunking and parallelization transparently. The Dask version uses far less memory than the naive version and finishes fastest (assuming you have CPUs to spare).
The way Dask works involves two steps:  
* First, you set up a computation, internally represented as a graph of operations.
* Then, you actually run the computation on that graph.


```python
import dask.dataframe as dd
# Load the data with Dask instead of Pandas.
df = dd.read_csv(
    "voters.csv",
    blocksize=16 * 1024 * 1024, # 16MB chunks
    usecols=["Residential Address Street Name ",
             "Party Affiliation "],

# Setup the calculation graph; unlike Pandas code,
# no work is done at this point:
def get_counts(df):
    by_party = df.groupby("Party Affiliation ")
    street = by_party["Residential Address Street Name "]
    return street.value_counts()
result = get_counts(df)

# Actually run the computation, using 2 threads:
result = result.compute(num_workers=2)

```

        

Dask isn’t a panacea, of course:

* Parallelism has overhead, it won’t always make things finish faster. And it doesn’t reduce the CPU time, so if you’re already saturating your CPUs it won’t speed things up on wallclock time either.
* Some tuning is needed. Larger block sizes increase memory use, but up to a point also allows faster processing.
If your task is simple or fast enough, single-threaded normal Pandas may well be faster.


### Datatable vs Rapids cuDF


If you have a huge dataset with millions of rows, don't use pandas to load. the datatable package which can read data up to 10 times faster. The datatable API for manipulating data may not be as intuitive as pandas - so, call the to_pandas method after reading the data to convert it to a DataFrame.

Even a 200k row dataset may exhaust your 16GB RAM while doing complex computations.Using the memory_usage method on a DataFrame with deep=True, we can get the exact estimate of how much RAM each feature is consuming. Now, there are certain tricks you can use to decrease memory usage up to 90%. These tricks have a lot to do with changing the data type of each feature to the smallest subtype possible.

`datatable` allows multi-threaded preprocessing of datasets sized up to 100 GBs. At such scales, pandas starts throwing memory errors while datatable humbly executes


 alternative is `cuDF`, developed by RAPIDS. This package has many dependencies and can be used in extreme cases (think hundreds of billions). It enables running preprocessing functions distributed over one or more GPUs, as is the requirement by most of today's data applications. Unlike datatable, its API is very similar to pandas.
 
 The only hassle when using cuDF is its installation — it requires:
* CUDA Toolkit 11.0+
* NVIDIA driver 450.80.02+
* Pascal architecture or better (Compute Capability >=6.0)

```python

import datatable as dt  # pip install datatable
import pandas as pd

%%time
tps_df = dt.fread("data/tps_september_train.csv").to_pandas()
tps_df.head()


memory_usage = tps_df.memory_usage(deep=True) / 1024 ** 2
memory_usage.head()
memory_usage.sum()

```

```python
import cudf, io, requests
from io import StringIO

url = "https://github.com/plotly/datasets/raw/master/tips.csv"
content = requests.get(url).content.decode('utf-8')

tips_df = cudf.read_csv(StringIO(content))
tips_df['tip_percentage'] = tips_df['tip'] / tips_df['total_bill'] * 100

# display average tip by dining party size
print(tips_df.groupby('size').tip_percentage.mean())

```

**References**
------------
[1] https://towardsdatascience.com/process-10m-row-datasets-in-milliseconds-in-this-comprehensive-pandas-speed-guide-5a3125cbb78  
[2] https://towardsdatascience.com/stop-using-csvs-for-storage-this-file-format-is-150-times-faster-158bd322074e  
[3] https://medium.com/featurepreneur/dask-pandas-for-faster-processing-7f9aa2f53cc7  
[4] https://towardsdatascience.com/stop-using-csvs-for-storage-here-are-the-top-5-alternatives-e3a7c9018de0  