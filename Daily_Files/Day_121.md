# Day 121 of ML 

## Pandas Tips and Tricks 

* **Loading the data** -  when you have large ccsv files, it takes a huge time to load especially when you have million rows. Don't use pandas to load it.  You can also try Dask, Vaex or cuDF

```python
import datatable as dt
df = dt.fread("data/train.csv").to_pandas()
```

* **Operations without loops** -  Instead of using apply function on multiple columns in a row using lambda, pull them out as df.values and create a new column by sending these arrays of the features as argument to a function.  Whenever you find yourself itching to use some looping function like apply, applymap, or itertuples - stop. Use vectorization instead. Pandas has a large collection of vectorized functions. 

```python
df["new"] = df.apply(lambda row : big_func(row["col1"],row["col2"],row["col3"],axis=1) 
df["new"] = big_func(df["col1"].values,df["col2"].values,df["col3"].values) 
```

* **Change data type too load quicker** - object type is a menace for categorical values and takes up too much space but that also happens for other data types. Here is a useful helper method . For other data types like datetime or timedelta, use the native formats offered in pandas since they enable special manipulation functions.

```python
def reduce_memory_usage(df, verbose=True):
    numerics = ["int8", "int16", "int32", "int64", "float16", "float32", "float64"]
    start_mem = df.memory_usage().sum() / 1024 ** 2
    for col in df.columns:
        col_type = df[col].dtypes
        if col_type in numerics:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == "int":
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)
            else:
                if (
                    c_min > np.finfo(np.float16).min
                    and c_max < np.finfo(np.float16).max
                ):
                    df[col] = df[col].astype(np.float16)
                elif (
                    c_min > np.finfo(np.float32).min
                    and c_max < np.finfo(np.float32).max
                ):
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
    end_mem = df.memory_usage().sum() / 1024 ** 2
    if verbose:
        print(
            "Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)".format(
                end_mem, 100 * (start_mem - end_mem) / start_mem
            )
        )
    return df
```


* **Style on describe for easy viewing** -  You can display stylized dataframes. Raw df are rendered as html tables with a bit of CSS inside jupyter 

```python
df.sample(20,axis=1).describe().T.style.bar(subset=['mean'],color='#205ff2').background_gradient(subset=['std'],cmaps='Reds').background_gradient(subset=['50%'],cmap='coolwarm')
```

* Save dataframes as feathers or parquets instead.
*  Pandas options - to tweak default behaviors

```python
import pandas as pd
dir(pd.options)
['compute', 'display', 'io', 'mode', 'plotting']

```

* `select_dtypes` -It has include and exclude parameters that you can use to select columns including or excluding certain data types

```python
df.select_dtypes(include=np.number).head()
df.select_dtypes(exclude=np.number).head()
```

* Value counts - you can choose to include nans and also normalize the frequency counts

```python
df.value_counts(dropna=False, normalize=True)
```

* Timestamps - `at_time` and `between_time` . These two can be useful when working with time series that have high granularity. at_time allows you to subset values at a specific date or time and between_time to select rows within a custom interval. When index is the timestamp:

```python
from datetime import datetime
data.at_time("15:00")
data.between_time("09:45", "12:00")

```

* `bdate_range` is a short-hand function to create TimeSeries indices with business-day frequency.Business-day frequencies are common in the financial world. So, this function may come in handy when reindexing existing time-series with reindex function.

```python
series = pd.bdate_range("2021-01-01", "2021-01-31")  # A period of one month
len(series) # 21
```


## inplaces

inplace=True
Using the inplace=True keyword in a pandas method changes the default behaviour such that the operation on the dataframe doesn’t return anything, it instead ‘modifies the underlying data’. It mutates the actual object which you apply it to.

This means that any other objects referencing this dataframe (such as slices) will now see the modified version of the data — not the original.

Imagine having an ice-cream, it’s frozen, but then you want to melt it. If you use inplace=True you change the state of the object. You can’t get your unmelted ice-cream back.

Alternatively, when using inplace=False (which is the default behaviour) the dataframe operation returns a copy of the dataframe, leaving the original data intact. 

`inplace=True` prevents the use of chaining because nothing is returned from the methods.

```python
def create_country_leaderboard(df):
    country_df = df.groupby("country")[["sales", "refunds"]].sum()
    country_df.rename(index=str.lower, inplace=True)
    country_df.reset_index(inplace=True)
    country_df.sort_values(by="sales", inplace=True)
    return country_df

def create_country_leaderboard(df):
    return (
        df.groupby("country")[["sales", "refunds"]]
          .sum()
          .rename(index=str.lower)
          .reset_index()
          .sort_values(by="sales")
    )

```

* If you forget to add the inplace=True to one of your lines (I did this when writing one of the examples), you might miss an operation on your dataframe and it can be hard to spot, because you’ll have a random line just not doing anything useful.
* If you use Jupyter notebooks, it will make it even harder to manage the state of different objects — if a function that mutates your dataframe ends up erroring half way through the function (also happened to me when writing an example), you will end up with a half mutated dataframe.

## Apply   

If you miss the chaining style that .apply() gives you, consider using .pipe() here instead, this has a much simpler usage pattern and is just as fast as the direct function call.
`test_df[["float_col1", "float_col2"]].pipe(trig_transform)`



# Dropping duplicate rows with the most nulls

```python
df = df.loc[df.notnull().sum(1).groupby(df["cid"]).idxmax()].reset_index(drop=True)
```

# Unable to get rid of the `<NA>` missing value

The error is related to the fact that some of the values in your pandas DataFrame are pd.NaT and will then cause troubles when calling json.dumps().

One possible solution is to replace all missing values (including None, pd.NaT, numpy.nan and really any other missing value-realted type) first with np.nan and then replace the latter with None:
```python
import numpy as np

df = df.fillna(np.nan).replace([np.nan], [None])
```

## Stackoverflow useful 

* [Long to wide reshape by two variables](https://stackoverflow.com/questions/22798934/pandas-long-to-wide-reshape-by-two-variables)


**References**
------------
[1] https://towardsdatascience.com/25-pandas-functions-you-didnt-know-existed-p-guarantee-0-8-1a05dcaad5d0  
[2] https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html  
