# Day 121 of ML 

## Pandas Tips and Tricks 

* **Loading the data** -  when you have large ccsv files, it takes a huge time to load especially when you have million rows. Don't use pandas to load it.  You cna also try Dask, Vaex or cuDF

```python
import datatable as dt
df = dt.fread("data/train.csv").to_pandas()
```

* **Operations without loops** -  Instead of using apply function on multiple columns in a row using lambda, pull them out as df.values and create a new column by sending these arrays of the features as argument to a function.  Whenever you find yourself itching to use some looping function like apply, applymap, or itertuples - stop. Use vectorization instead. Pandas has a large collection of vectorized functions. 

```python
df["new"] = df.apply(lambda row : big_func(row["col1"],row["col2"],row["col3"],axis=1) 
df["new"] = big_func(df["col1"].values,df["col2"].values,df["col3"].values) 
```

* **Change data type too load quicker** - object type is a menace for categorical values and takes up too much space but that also happens for other data types. Here is a useful helper method . For other data types like datetime or timedelta, use the native formats offered in pandas since they enable special manipulation functions.

```python
def reduce_memory_usage(df, verbose=True):
    numerics = ["int8", "int16", "int32", "int64", "float16", "float32", "float64"]
    start_mem = df.memory_usage().sum() / 1024 ** 2
    for col in df.columns:
        col_type = df[col].dtypes
        if col_type in numerics:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == "int":
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)
            else:
                if (
                    c_min > np.finfo(np.float16).min
                    and c_max < np.finfo(np.float16).max
                ):
                    df[col] = df[col].astype(np.float16)
                elif (
                    c_min > np.finfo(np.float32).min
                    and c_max < np.finfo(np.float32).max
                ):
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
    end_mem = df.memory_usage().sum() / 1024 ** 2
    if verbose:
        print(
            "Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)".format(
                end_mem, 100 * (start_mem - end_mem) / start_mem
            )
        )
    return df
```


* **Style on describe for easy viewing** -  You can display stylized dataframes. Raw df are rendered as html tables with a bit of CSS inside jupyter 

```python
df.sample(20,axis=1).describe().T.style.bar(subset=['mean'],color='#205ff2').background_gradient(subset=['std'],cmaps='Reds').background_gradient(subset=['50%'],cmap='coolwarm')
```

* Save dataframes as feathers or parquets instead.
*  Pandas options - to tweak default behaviors

```python
import pandas as pd
dir(pd.options)
['compute', 'display', 'io', 'mode', 'plotting']

```

* `select_dtypes` -It has include and exclude parameters that you can use to select columns including or excluding certain data types

```python
df.select_dtypes(include=np.number).head()
df.select_dtypes(exclude=np.number).head()
```

* Value counts - you can choose to include nans and also normalize the frequency counts

```python
df.value_counts(dropna=False, normalize=True)
```

* Timestamps - `at_time` and `between_time` . These two can be useful when working with time series that have high granularity. at_time allows you to subset values at a specific date or time and between_time to select rows within a custom interval. When index is the timestamp:

```python
from datetime import datetime
data.at_time("15:00")
data.between_time("09:45", "12:00")

```

* `bdate_range` is a short-hand function to create TimeSeries indices with business-day frequency.Business-day frequencies are common in the financial world. So, this function may come in handy when reindexing existing time-series with reindex function.

```python
series = pd.bdate_range("2021-01-01", "2021-01-31")  # A period of one month
len(series) # 21
```


**References**
------------
[1] https://towardsdatascience.com/25-pandas-functions-you-didnt-know-existed-p-guarantee-0-8-1a05dcaad5d0  
[2] 