# Day 59 of ML 

### The Engineering Problem of A/B Testing

A/B testing, or multivariate testing, is a mechanism to compare two (or more) versions of the same feature or page, and compare the statistics of those versions to see which one performs better. Ideally, this leads to more (data-)informed decision making, and enables fast feedback loops and continuous improvement. 

        Isolate experiments as much as possible

At a high-level, this is how it works (or, how it should work):
* A hypothesis is formed about user behavior (often based on psychological theories, like Fear of missing out). To test this hypothesis, we create an experiment, where we test the current implementation (often called “Control”), and one or more variations of a new implementation. Before the test starts, a decision is made on what metrics the experiment will be evaluated.
* A user accesses the website or application, and specifically the feature you are testing.
* They are then bucketed, meaning they are assigned to a variant of the experiment you are testing. In which bucket the user lands, determines which version of the experiment they get to see and use. They stay in this bucket for the duration of the experiment.
* User behavior events are recorded, and stored for statistical analysis. Usually, the experiments are active for at least a week or longer. As soon as they are completed, the results are analyzed, and in most cases, the best performing variant of the experiment is rolled out to all users.

What you need
* **Bucketing strategy** -  50:50 or 80:10:10 
* **Remote configuration** - turn experiments off and on with a configuration that is separated from code. At the very least, you’ll want to manage which experiments are enabled (because, you know, things break), as well as things like start and end dates, or targeting.
* **Tracking**: 
    * get data about user behavior in our experiment
    * which variant performed better. 
    * How much time do our users spend on the page? 
    * Do they convert to the next step in the funnel?
    * How many shopping items do they add to their basket? 
* **Visual editors**: Tools like Optimizely offer a visual editor that allows you to click your way to designing a new experiment  

You’ll need to hook into these events, record it and persist it in a central location for analysis. 

### Implementation approaches

1) Canary Releases - deploy a new version per variant that you want to test (a feature branch perhaps), and then route a subset of your users (with sticky sessions) to that new deployment.
    * you have to have a well-managed infrastructure and release pipeline, especially when you want to run multiple tests in parallel and need many different deployments and the routing complexities that come with it.
    *  you’ll need a decent amount of traffic, too.
    *  any failed experiment does not introduce technical debt (code never lands on master, and deployments are just deleted)
    * this enforces that a user can only be in one experiment at a time; multiple experiments introduce both technical challenges and uncertainty about experiments influencing each other.
2) **Split URLs** - like /amazing-feature/test-123/b. Historically recommended by Google to prevent SEO issues, you can use URLs to route users to different experiments.  
    * Benefit is that you will not negatively impact any SEO value a given URL on your domain has while you’re experimenting with different designs.
3) **Server Side** - A cookie is then set to ensure the user is “stuck” in this bucket, and it’s used to render an interface with whatever experiments the user is in.
    * For the user, this is one of the best options, because the performance impact is negligible. 
    * However, because you use cookies, the benefits of a CDN are limited. Cookies introduce variation in requests (especially if users can enter multiple experiments), and it will lead to cache misses, leaving you without the protection of a CDN.
4) **Client side** want to have maximum flexibility, client-side A/B testing is also an option. In this scenario, either no interface is rendered, or the original interface, and as soon, or slightly before this happens, the experiments are activated, and the interface is augmented based on whatever variant the user is in. 
    *   However, it’s often the worst choice in terms of performance.
    *   
5) On the Edge - If you have a CDN in front of your website, you can use the power of edge workers to run experiments.
    * the gist of it is that your server renders all variations of your interface, your CDN caches this response, and then when a user loads your website, the cached response is served, after the edge worker removes the HTML that is not applicable to the user that requests it. 
    * A very promising approach if you care about performance, because you get the benefits of a CDN without any impact on browser performance

### Reality 

we cannot afford the luxury of one experiment per user across the platform, due to a lack of traffic. In practice, this means that experiments have side-effects. 

* Concurrently implemented experiments make any reasonable expectation of end-to-end test coverage impossible
* Running multiple experiments across a user’s journey will lead to uncertainty about test results. 

you might want to consider mutually exclusive experiments — meaning that a user that is in experiment X cannot be in experiment Y at the same time. This will help eliminate behavioral side-effects.

The best isolation strategy I can think of is **canary releases and feature branches**

1) create a branch that will contain the changes for a variant
2) open a pull request, and a test environment with that change is deployed
3) Once it passes review, it is then deployed to the production, and the router configuration is updated to sent a certain amount of traffic to the variant that you want to test
4) look at expected usage, general traffic and a desired duration of the test to determine what traffic split makes sense
5) If you estimate 20% of traffic for a week would be enough, exclude 80% of traffic for the test, and split the remaining 20% evenly over an instance that is running the current version of your website, and a version that is running a variant of the experiment.
6) You have to have enough traffic here, and at this point you will see benefits from splitting up your website in smaller deployable units. For instance, you might want to consider splitting up your frontend in micro-frontends.
7) If you need more testing confidence, you can opt-in for lower level testing, like unit or component testing.

![alt_text](https://miro.medium.com/max/1015/1*4j4wZDdakosnMPfvhsmwOw.png)

**References**
------------
[1]  https://levelup.gitconnected.com/the-engineering-problem-of-a-b-testing-ac1adfd492a8  
[2]