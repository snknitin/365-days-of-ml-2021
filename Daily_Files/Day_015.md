# Day 15 of ML 

Install the following 
```python
pip install contractions
pip install distilbert-punctuator
pip install textstat
pip install gibberish-detector
pip install nlpaug

```

* **Contractions** will help you avoid write a long list of regular expressions to expand contractions in your text data (i.e. don’t → do not; can’t → cannot; haven’t → have not).
  * An important part of text preprocessing is creating uniformity and whittling down the list of unique words without losing too much meaning. 
  * For instance, bag-of-words models and TF-IDF create large sparse matrixes, in which each variable is a distinct vocabulary word in the corpus. Expanding contractions can further reduce dimensionality, or even help filter out stopwords.

* **Distilbert-punctuator** is the only working Python library that can restore missing punctuation to plain English text
    * Sometimes, you simply want your text data to be more grammatically correct and presentable. Whether the task is fixing messy Twitter posts or chatbot messages, this library is for you.

* **Textstat** is an easy-to-use, lightweight library that provides various metrics on your text data, such as reading level, reading time, and word count.

*  **gibberish-detector** to help me remove bad observations from datasets.It can also be implemented for error handling on user inputs. For instance, you may want to return an error message if a user enters meaningless, gibberish text on your web app
   
        * you will also need to train the model on your end, but it’s very simple and only takes a minute. Just follow these steps:
        * Download the training corpus called big.txt from here
        * Open your CLI and cd over to the directory in which big.txt is located
        * Run the following: gibberish-detector train .\big.txt > gibberish-detector.model
        A file called gibberish-detector.model will be created in your current directory.  
        
* **NLPAug** - The library can augment text by either substituting or inserting words that are semantically associated. 
    * on a dataset that has 15k positive reviews, and only 4k negative reviews. A heavily imbalanced dataset such as this will create model bias towards the majority class (positive reviews) during training.
    * Simply duplicating examples of the minority class (negative reviews) will not add any new information to the model. Instead, utilize the advanced text augmentation features of NLPAug to increase the minority class with variety. This technique has been shown to improve AUC and F1-Score

## Usage 
   
```python
import contractions
s = "ive gotta go! i'll see yall later."
text = contractions.fix(s, slang=True)
print(text) # I have got to go! I will see you all later.
```

```python
from dbpunctuator.inference import Inference, InferenceArguments
from dbpunctuator.utils import DEFAULT_ENGLISH_TAG_PUNCTUATOR_MAP
args = InferenceArguments(
        model_name_or_path="Qishuai/distilbert_punctuator_en",
        tokenizer_name="Qishuai/distilbert_punctuator_en",
        tag2punctuator=DEFAULT_ENGLISH_TAG_PUNCTUATOR_MAP
    )
punctuator_model = Inference(inference_args=args, 
                             verbose=False)
text = [
    """
however when I am elected I vow to protect our American workforce
unlike my opponent I have faith in our perseverance our sense of trust and our democratic principles will you support me
    """
]
print(punctuator_model.punctuation(text)[0])
# However, when I am elected, I vow to protect our American workforce.
# Unlike my opponent, I have faith in our perseverance, 
# our sense of trust and our democratic principles. Will you support me?
```

```python
import textstat
text = """ Love this dress! it's sooo pretty.
 i happened to find it in a store, and i'm glad i did
 bc i never would have ordered it online bc it's petite. 
"""
# Flesch reading ease score
print(textstat.flesch_reading_ease(text))
  # 90-100 | Very Easy
  # 80-89  | Easy
  # 70-79  | Fairly Easy
  # 60-69  | Standard
  # 50-59  | Fairly Difficult
  # 30-49  | Difficult
  # <30    | Very Confusing
# Reading time (output in seconds)
# Assuming 70 milliseconds/character
print(textstat.reading_time(text, ms_per_char=70))
# Word count 
print(textstat.lexicon_count(text, removepunct=True))
```

```python
from gibberish_detector import detector
# load the gibberish detection model
Detector = detector.create_from_model('.\gibberish-detector.model')
text1 = "xdnfklskasqd"
print(Detector.is_gibberish(text1))
text2 = "apples"
print(Detector.is_gibberish(text2))
```

```python
import nlpaug.augmenter.word as naw
# main parameters to adjust
ACTION = 'substitute' # or use 'insert'
TOP_K = 15 # randomly draw from top 15 suggested words
AUG_P = 0.40 # augment 40% of words within text
aug_bert = naw.ContextualWordEmbsAug(
    model_path='bert-base-uncased', 
    action=ACTION, 
    top_k=TOP_K,
    aug_p=AUG_P
    )
text = """
Come into town with me today to buy food!
"""
augmented_text = aug_bert.augment(text, n=3) # n: num. of outputs
print(augmented_text)
```


**References**
------------
[1]  https://towardsdatascience.com/5-lesser-known-python-libraries-for-your-next-nlp-project-ff13fc652553
[2]