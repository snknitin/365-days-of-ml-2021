# Day 20 of ML 

## Pytorch Puzzles and basics 

```python
import torch
from torchtyping import TensorType as TT

tensor = torch.tensor

numpy_to_torch_dtype_dict = {
    bool: torch.bool,
    np.uint8: torch.uint8,
    np.int8: torch.int8,
    np.int16: torch.int16,
    np.int32: torch.int32,
    np.int64: torch.int64,
    np.float16: torch.float16,
    np.float32: torch.float32,
    np.float64: torch.float64,
}
torch_to_numpy_dtype_dict = {v: k for k, v in numpy_to_torch_dtype_dict.items()}

def arange(i: int):
    "Think for-loop"
    return torch.tensor(range(i))


def where(q, a, b):
    "Think if-statement"
    return (q * a) + (~q) * b
    
    
def ones(i: int) -> TT["i"]:
    # checking equality between 2 arange functions will give you a [True,True,...True] tensort that you can convert to ones
    return 1 * (arange(i) == arange(i))  
    
def sum(a: TT["i"]) -> TT[1]:
    # Matrix multiplication against a bunch of ones will give you the sum
    return a @ ones(a.shape[0])
    
def outer(a: TT["i"], b: TT["j"]) -> TT["i", "j"]:
    # Outer product of a tensor can be done bym multipling with the transpose to get n x n
    # b[None] will make it 2d tensor [1,1] -> [[1,1]] basically 1xn
    # a[:,None] makes it nx1 from size n
    return a[:, None] @ b[None] 
    
def diag(a: TT["i", "i"]) -> TT["i"]:
    # To access only the diagonal elements, you can use arange again to have (i,i) indices
    return a[arange(a.shape[0]), arange(a.shape[0])]
    
def eye(j: int) -> TT["j", "j"]:
    # To form the identity matrix, use the same  trick as the ones but with an outer broadcast boolean comparison
    return 1 * (arange(j)[:, None] == arange(j))
    
def triu(j: int) -> TT["j", "j"]:
    # For upper triangular matrix, just it should be 1 if i<=j else 0
    return 1 * (arange(j)[:, None] <= arange(j))

def cumsum(a: TT["i"]) -> TT["i"]:
    # To get cumulative sum of a tensor, do a matrix multiplication with a triu
    return a @ triu(a.shape[0])
    
def diff(a: TT["i"], i: int) -> TT["i"]:
    # To get running difference
    # arange(i)-1 will help shift the diagonal to the right in the identity matix
    # Subtracting from identity will give you prev row minus current row
    # matrix multiplication will get the difference
    return a @ (eye(i) - 1 * (arange(i)[:, None] == arange(i) - 1))

def vstack(a: TT["i"], b: TT["i"]) -> TT[2, "i"]:
    # Vertical stack 
    # condition arange(2)[:, None] == 1, will give you [[False],[True]], so it will pick second vector in first row
    # then first vector in second row. so switch a,b to b,a
    # Doing outer with ones will replicate the tensor into 2 rows
    return where(arange(2)[:, None] == 1, outer(ones(2), b), outer(ones(2), a))
    
def roll(a: TT["i"], i: int) -> TT["i"]:
    # Circular shift
    return a[arange(i) - i + 1]
    
def flip(a: TT["i"], i: int) -> TT["i"]:
    # Reverse flip by indexing
    return a[-1 - arange(i)]
    
def compress(g: TT["i", bool], v: TT["i"], i:int) -> TT["i"]:
    # Given a boolean and a value tensor, only select elements of the value base don boolean and compress them to the left
    # convert boolean to number, then take cumsum, and multiply that element wise with G to make the false 0, so [1,1,0,1] becomes [1,2,0,3]
    # make that into a 2d tendor with a new axis and compare with a shifted arange
    # compare only those cases that match and make a ones matrix to do an matrix multiplication and shifts the selected elements to the left
    return v @ (1 * ((cumsum(1 * g) * g)[:, None] == arange(i) + 1))


def pad_to(a: TT["i"], i: int, j: int) -> TT["j"]:
    # Create a matrix of ones with additional 0's and do matrix multiplication
    # this means diagonal matrix of 1 with a concatenated column of 0 till j
    return a @ (1 * (arange(i)[:, None] == arange(j)))
    
def sequence_mask(values: TT["i", "j"], length: TT["i", int]) -> TT["i", "j"]:
    # wherever in the row of the matrix, you reach the given spec of the length for the row, choose x, else replace with 0
    # length[:, None] - 1 will give us the index of each row where we need to stop sequence
    # Choose each column and decide the stop point in the row to make a matrix
    return where(arange(values.shape[1]) <= length[:, None] - 1, values, 0)
    
def bincount(a: TT["i"], j: int) -> TT["j"]:
    # Need to count occurence of numbers and keep tally in the index location of that number
    # first part is a ones tensor of size 1xn
    # Transpose the vector and compare it to the full max range of indices. This will make a vector of size n x (max_idx) which has 1 every occurence
    # doing a matrix multiplication will sum the bin counts in each location
    return (ones(a.shape[0])[None] @ (1 * (a[:, None] == arange(j)[None])))[0]

```

[torch.where](https://pytorch.org/docs/stable/generated/torch.where.html#torch.where)

![alt_text](http://blog.ezyang.com/img/pytorch-internals/slide-06.png)

How do I translate this logical position into a location in physical memory? Strides tell me how to do this: to find out where any element for a tensor lives, I multiply each index with the respective stride for that dimension, and sum them all together.

Using advanced indexing support, I can just write tensor[1, :] to get this row. Here's the important thing: when I do this, I don't create a new tensor; instead, I just return a tensor which is a different view on the underlying data. This means that if I, for example, edit the data in that view, it will be reflected in the original tensor.

When we look at the physical memory, we see that the elements of the column are not contiguous: there's a gap of one element between each one. Here, strides come to the rescue: instead of specifying a stride of one, we specify a stride of two, saying that between one element and the next, you need to jump two slots. (By the way, this is why it's called a "stride": if we think of an index as walking across the layout, the stride says how many locations we stride forward every time we take a step.)

![alt_text](http://blog.ezyang.com/img/pytorch-internals/slide-10.png)

[Stride Visualizer](https://ezyang.github.io/stride-visualizer/index.html)

There may be multiple tensors which share the same storage. Storage defines the dtype and physical size of the tensor, while each tensor records the sizes, strides and offset, defining the logical interpretation of the physical memory.

One thing to realize is that there is always a pair of Tensor-Storage, even for "simple" cases where you don't really need a storage (e.g., you just allocated a contiguous tensor with torch.zeros(2, 2)).

**References**
------------
[1]  http://blog.ezyang.com/2019/05/pytorch-internals/  
[2]  https://github.com/srush/Tensor-Puzzles
