# Day 20 of ML 

## Pytorch Puzzles and basics 

```python
import torch
from torchtyping import TensorType as TT

tensor = torch.tensor

numpy_to_torch_dtype_dict = {
    bool: torch.bool,
    np.uint8: torch.uint8,
    np.int8: torch.int8,
    np.int16: torch.int16,
    np.int32: torch.int32,
    np.int64: torch.int64,
    np.float16: torch.float16,
    np.float32: torch.float32,
    np.float64: torch.float64,
}
torch_to_numpy_dtype_dict = {v: k for k, v in numpy_to_torch_dtype_dict.items()}

def arange(i: int):
    "Think for-loop"
    return torch.tensor(range(i))


def where(q, a, b):
    "Think if-statement"
    return (q * a) + (~q) * b
    
    
def ones(i: int) -> TT["i"]:
    # checking equality between 2 arange functions will give you a [True,True,...True] tensort that you can convert to ones
    return 1 * (arange(i) == arange(i))  
    
def sum(a: TT["i"]) -> TT[1]:
    # Matrix multiplication against a bunch of ones will give you the sum
    return a @ ones(a.shape[0])
    
def outer(a: TT["i"], b: TT["j"]) -> TT["i", "j"]:
    # Outer product of a tensor can be done bym mtulitpling with the transpose to get n x n
    return a[:, None] @ b[None] 
    
def diag(a: TT["i", "i"]) -> TT["i"]:
    # To access only the diagonal elements, you can use arange again to have (i,i) indices
    return a[arange(a.shape[0]), arange(a.shape[0])]
    


```


![alt_text](http://blog.ezyang.com/img/pytorch-internals/slide-06.png)

How do I translate this logical position into a location in physical memory? Strides tell me how to do this: to find out where any element for a tensor lives, I multiply each index with the respective stride for that dimension, and sum them all together.

Using advanced indexing support, I can just write tensor[1, :] to get this row. Here's the important thing: when I do this, I don't create a new tensor; instead, I just return a tensor which is a different view on the underlying data. This means that if I, for example, edit the data in that view, it will be reflected in the original tensor.

When we look at the physical memory, we see that the elements of the column are not contiguous: there's a gap of one element between each one. Here, strides come to the rescue: instead of specifying a stride of one, we specify a stride of two, saying that between one element and the next, you need to jump two slots. (By the way, this is why it's called a "stride": if we think of an index as walking across the layout, the stride says how many locations we stride forward every time we take a step.)

![alt_text](http://blog.ezyang.com/img/pytorch-internals/slide-10.png)

[Stride Visualizer](https://ezyang.github.io/stride-visualizer/index.html)

There may be multiple tensors which share the same storage. Storage defines the dtype and physical size of the tensor, while each tensor records the sizes, strides and offset, defining the logical interpretation of the physical memory.

One thing to realize is that there is always a pair of Tensor-Storage, even for "simple" cases where you don't really need a storage (e.g., you just allocated a contiguous tensor with torch.zeros(2, 2)).

**References**
------------
[1]  http://blog.ezyang.com/2019/05/pytorch-internals/  
[2]  https://github.com/srush/Tensor-Puzzles
