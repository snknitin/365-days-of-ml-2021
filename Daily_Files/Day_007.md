# Day 7 of ML 


## Data Skeptic Podcast Mini Notes

* **p values** - During hypothesis testing, whatever you seek to prove will be the alternative hypothesis and the opposite of that is going to be our null hypothesis. We either accept or reject the null hypothesis which indirectly means the opposite for the alternative. To ensure that the samples you observe aren't anecdotal and sufficient to derive a conclusion, we split them into two groups or several depending on how many variables we are trying to account for. We want to ensure that the difference between the test and control groups are statistically significant and not just random noise. Ergo, to be able to accept the premise you need a sufficient amount of confidence interval that will satisfy you, like 90% or 95% certainty that this result is not due to random noise. Thinking of this in reverse. you are willing to tolerate only an error or 5-10% which is your `alpha` value. You want your p value to be less than the alpha which means your experimental conclusion is the same as your alternative hypothesis and you reject the null. If your p value is greater, that means you have more error than you can tolerate and so your experiment is a failure .

* **Bonferroni correction** -  Given that at a point you may be testing for teh effect of one feature on the target variable but if there are multiple number of features involved each having it's own effect and tolerance threshold for alpha then you need to modify the tolerance by making a correction which is usually dividing alpha by the number of variables involved  
 
* **z scores** - This score will tell you how many standard deviations away you are from the mean. It may be similar to percentile

* **T-test** - 


* 

**References**
------------
[1]  
[2]