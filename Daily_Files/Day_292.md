# Day 292 of ML 

# Gradients are not all you need


Back propagation through time cannot be done in all cases. Think of autoencoders where you send an input and learn the latent representation. However in variational auto encoders, instead of just a latent vector you learn a set of means and stds of the features in the latent space so as to sample new latent representations and feed it to a decoder. The reconstruction loss + KL divergence can then be used to backprogagate the decoder  params but this sampling step in between at the laten stpe is not recoverable or differentiable. 

This is where the reparameterization trick , utilizing the interesting aspect of gaussians, comes in handy where you sample `x` from `Normal distribution  N ~(0,1)` and sample `y` from `N~(mu,sigma)` then the relationship between x and y is that `y= mu*x + sigma` . You cna sample a value from normal and multiple with the statistics of the other distribution. This gives a different route for the backprop 

Due to this sampling nature there is a smoothening factor of loss functions but the loss being differentiated is not because we created a deterministic route using re-parameterization. This will screw up the gradients. It will be unbiased but variance will be super high. 

Backprop through dynamic systems may not always be a good idea -
* Rigid body physics
* RL -  Sampling actions 
* Meta Learned Optimizers -  external and internal optimizers  
* Molecular dynamics - packing
 
 
 If you shift some parameter slightly the loss will vary a lot. These parameters are dynamically sampled and the variance of gradients will explode 

The matrix of partial derivatives of the loss is the jacobian and has an iterated structure . So the gradients of loss functions ina  dynamic system depend intimately on the spectra of the jacobian. The jacobian can be decomposed into eigen spectrum as two transformations with the diagonal matrix in the center and the largest eigen value on top of the diagonal.  if this is >1 then we have exploding gradients and if less than 1 we have vanishing gradients. Whatever vector you have initially is going to explode or shrink based on this. 

There are some solutions in the paper to mitigate these effects. 

**References**
------------
[1]  https://www.youtube.com/watch?v=EeMhj0sPrhE 
[2]