# Day 61 of ML 

## Transformer neural networks


### Input

1) Take all the words present in the vocabulary and make a dictionary out of it and assign a numeric index next to each word. 
2) Then you pick only the words that occur in the current input text and what gets fed into the transformer are the indices.
    * Against each of these indices a vector is attached to form an embedding that are initially filled with random numbers and they get updated in the training phase to better suit the tasks. The paper takes embedding size as 512
    * Each dimension tries to capture some linguistic feature about the word in the input. It is non-trivial to find out exactly what info is encoded like POS tags or NER etc in a particular dimension since it is decided during the training.
    * Graphically the values of teh dimensions in the embeddings represent coordinates of words in hyperspace. If two words are similar and appear in same contexts their embedding values are updated to become closer and closer during the training process. 
3)  

### Position embeddings

If an LSTM were to take these embeddings, it would take them one at a time sequentially and this is why they are slow. they may know which word came first or the ordering of the words in the sequence.

Transformers on the other hand take up all the embeddings at once. Downside is they may lose critical information related to word ordering. This is important because a word like "not" cna change the whole meaning of a sentence depending on where it is. 

To get word order info, we add a new set of vectors by adding them to the input embeddings called positional embeddings. Position vector of a should remain the side regardless of the size of the text so it should not be indexed in a way that adding them would change the embeddings of the words. 

Size of the position embedding should be same as the word embeddings. so 512. 

We use **"Wave Frequencies"** to capture position info. If you plot a sinusoidal curve by varying the variable indicating word positions on the x-axis we get a smooth looking curve. Height of the sine curve depends on the word position on the x-axis. 

This might still cause problems because of the cyclic nature. so words at regular intervals can be at the same height. If 2 points are close by on the curve they will remain identical at higher frequencies too. it is only on much higher frequencies tha their height on the y coordinates will differ and you can tell them apart.  For points further apart in the initial curve , you should be able to see the difference in heights quite early on as you increase the frequency. 

Therefore both the position and the embedding dimension will inform us about the word order. 
The authors use a combination of sine and cosine curves. Odd positions use the sine formula and even positions use the cosine formula. 


## Multi-head attention layer

**Self Attention** to tackle word sense disambiguation. 


**References**
------------
[1]  https://www.youtube.com/watch?v=dichIcUZfOw&list=TLPQMDYwMzIwMjEV-T1sCdzl8A&index=2  
[2]