# Day 61 of ML 

## Transformer neural networks


### Input

1) Take all the words present in the vocabulary and make a dictionary out of it and assign a numeric index next to each word. 
2) Then you pick only the words that occur in the current input text and what gets fed into the transformer are the indices.
    * Against each of these indices a vector is attached to form an embedding that are initially filled with random numbers and they get updated in the training phase to better suit the tasks. The paper takes embedding size as 512
    * Each dimension tries to capture some linguistic feature about the word in the input. It is non-trivial to find out exactly what info is encoded like POS tags or NER etc in a particular dimension since it is decided during the training.
    * Graphically the values of teh dimensions in the embeddings represent coordinates of words in hyperspace. If two words are similar and appear in same contexts their embedding values are updated to become closer and closer during the training process. 


### Position embeddings

If an LSTM were to take these embeddings, it would take them one at a time sequentially and this is why they are slow. they may know which word came first or the ordering of the words in the sequence.

Transformers on the other hand take up all the embeddings at once. Downside is they may lose critical information related to word ordering. This is important because a word like "not" cna change the whole meaning of a sentence depending on where it is. 

To get word order info, we add a new set of vectors by adding them to the input embeddings called positional embeddings. Position vector of a should remain the side regardless of the size of the text so it should not be indexed in a way that adding them would change the embeddings of the words. 

Size of the position embedding should be same as the word embeddings. so d = 512.

![alt_text](https://github.com/snknitin/365-days-of-ml-2021/blob/main/static/D60_1.PNG) 

We use **"Wave Frequencies"** to capture position info. If you plot a sinusoidal curve by varying the variable indicating word positions on the x-axis we get a smooth looking curve. Height of the sine curve depends on the word position on the x-axis.   

![alt_text](https://github.com/snknitin/365-days-of-ml-2021/blob/main/static/D60_2.PNG) 

This might still cause problems because of the cyclic nature. so words at regular intervals can be at the same height. 


If 2 points are close by on the curve they will remain identical at higher frequencies too. it is only on much higher frequencies tha their height on the y coordinates will differ and you can tell them apart.  

![alt_text](https://github.com/snknitin/365-days-of-ml-2021/blob/main/static/D60_3.PNG) 

For points further apart in the initial curve , you should be able to see the difference in heights quite early on as you increase the frequency. 

![alt_text](https://github.com/snknitin/365-days-of-ml-2021/blob/main/static/D60_4.PNG) 

Therefore both the position and the embedding dimension will inform us about the word order. 
The authors use a combination of sine and cosine curves. Odd positions use the sine formula and even positions use the cosine formula. 


## Multi-head attention layer

**Self Attention** to tackle word sense disambiguation. The difference between Attention and Self-Attention is that the simple attention module helps the model focus on the important words in the sentence with respect to some external query.
 
 The more important a word is to answer that query, the more importance it is given. Self attention considers the context and the relationship with the surrounding words. Every word can be regarded as the sum of the words it pays most attention to. 


We have 3 linear layers in the input where we replicate the position aware input embeddings it into (Q,K,V) query key value triples. This is motivated from retrieval of information. Similarity is a proxy to attention. To find the best match we use cosine similarity between Q and K. The dot product gives us the attention filter. This gets scaled(Divided by the square root of dimension) and then  passes through a softmax to get attention scores.


The reason for doing this can eb intuitively understood in terms of Computer Vision. Processing all info pixel by pixel is too slow. Filtering out unnecessary background image and focusing on what matters in that moment and context. Final filtered image is a attention filter multiplied with the original image. So when we multiple the value(input embedding) with the attention filter
we get a filtered value matrix which assigns high focus to features that are more important. 

![alt_text](https://github.com/snknitin/365-days-of-ml-2021/blob/main/static/D60_5.PNG) 

The filtered value is the final output of the mutli-head attention layer. You need multiple heads similar to this. You may need focus on mutliple aspects at the same time. They learn multiple attention filters and output filtered values. The paper uses 8 heads. These values are then concatenated
 
##  Decoder's masked attention

**Residual Connections** - serve 2 main purposes. Knowledge preservation and Vanishing Gradient problem. Inputs get considerably modified as they pass through forward propagation layers by the time they reach the ending of the network resulting in the loss of useful info. Adding a skipped connection to add the input again to the layers solves it.  
 
 
**Layer Normalization** - Add and Norm layer. Add part is just adding teh matrices of the emb+pos and the output of the multi-head attention module. Normalizing this result makes convergence faster and more stable. rows are words in the input and columns are features.

![alt_text](https://github.com/snknitin/365-days-of-ml-2021/blob/main/static/D60_6.PNG) 


The encoder takes the input text and converts it into a representation. Decoder takes this and converts it into output or the new text. Decoder takes 2 inputs. the output of the encoder and the output text that has been generated thus far. Similar to LSTM. First token the decoder generates is the START token and we feed this to the decoder and it travels through the output embedding layer which converts it into a vector with pos emb.

The encoded output from the encoder is split into Q and K while the decoder creates a V which goes into the multi-head of the decoder

![alt_text](https://github.com/snknitin/365-days-of-ml-2021/blob/main/static/D60_7.PNG) 

Size of the final linear layer in the decoder depends on the number of classes there are. if this is a text generation task then the final layer will be the same size as the vocabulary. Softmax will give us scalar probability scores.

We flatten the Filtered matrix from the multi-head of the decoder into a single row and is concatenated to pass through the linear feed forward layers.


We start with the START token and let the decoder generate the first word and then this is concatenated as the input embedding and passed through the decoder again, concatenating it each time till the END token is generated. 

**Masked-attention**

Unlike Inference where we don't know the answer to the input before hand, and during training phase the model gets provided with both the input source and the output target. 

This target is masked. Why? A teacher doesn't straight away show you all teh answers. Then it would just be memorization and not learning. After getting the encoder output and the START token, the decoder generates the first token output. We then unmasked and show the model the actual word it should have generated.

 
Now unlike inference where we pass the decoder's own predictions back to it during training, we pass the correct target word to the decoder. This is called **Teacher Forcing**. This helps the model in quantifying the difference between the probability distribution scores of the prediction and the truth. One way to quantify this is to use Cross Entropy Loss. 

![alt_text](https://github.com/snknitin/365-days-of-ml-2021/blob/main/static/D60_8.PNG) 

Future words are given a -inf which become zero as they pass through the softmax. This way the model only pays attention to all the words before the current token and not the rest of the target sentence.
 
 At 0 timestep, model pays attention only to the encoder output and the special start token which is the same as the ned token of the encoder. 
 
 ![alt_text](https://github.com/snknitin/365-days-of-ml-2021/blob/main/static/D60_9.PNG)
 ![alt_text](https://github.com/snknitin/365-days-of-ml-2021/blob/main/static/D60_10.PNG)
 
 
**References**
------------
[1]  https://www.youtube.com/watch?v=dichIcUZfOw&list=TLPQMDYwMzIwMjEV-T1sCdzl8A&index=2  
[2]