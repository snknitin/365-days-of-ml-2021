# Day 291 of ML 

## PyTorch -  Things you should know

I already made some notes [here](https://github.com/snknitin/PyTorch_learn)

* Consider checkpointing. You can trade-off memory for compute by using checkpoint. PyTorch uses a caching memory allocator to speed up memory allocations. As a result, the values shown in nvidia-smi usually don’t reflect the true memory usage

* If your GPU memory isn’t freed even after Python quits, it is very likely that some Python subprocesses are still alive. You may find them via `ps -elf | grep python` and manually kill them with `kill -9 [pid]`.

* Don’t accumulate history across your training loop. Convert some differentiable variables with autograd hisory into floats if you are just accumulating the value 

*  Don’t hold onto tensors and variables you don’t need.  You will get the best memory usage if you don’t hold onto temporaries you don’t need. Use `del variable_name`


* Avoid running RNNs on sequences that are too large. The amount of memory required to backpropagate through an RNN scales linearly with the length of the RNN input; thus, you will run out of memory if you try to feed an RNN a sequence that is too long.  implement `truncated BPTT`

* Don’t use linear layers that are too large. A linear layer nn.Linear(m, n) uses O(nm)O(nm) memory: that is to say, the memory requirements of the weights scales quadratically with the number of features. It is very easy to blow through your memory this way (and remember that you will need at least twice the size of the weights, since you also need to store the gradients.)

* when you do run out of memory, your recovery code can’t allocate either. That’s because the python exception object holds a reference to the stack frame where the error was raised. Which prevents the original tensor objects from being freed. The solution is to move you OOM recovery code outside of the except clause. Instead of 

```python
try:
    run_model(batch_size)
except RuntimeError: # Out of memory
    for _ in range(batch_size):
        run_model(1)

# Do this 
oom = False
try:
    run_model(batch_size)
except RuntimeError: # Out of memory
    oom = True

if oom:
    for _ in range(batch_size):
        run_model(1)

```

* You are likely using other libraries to generate random numbers in the dataset and worker subprocesses are started via fork. See torch.utils.data.DataLoader’s documentation for how to properly set up random seeds in workers with its `worker_init_fn` option.

* `torch.multiprocessing` is a drop in replacement for Python’s multiprocessing module. It supports the exact same operations, but extends it, so that all tensors sent through a multiprocessing.Queue, will have their data moved into shared memory and will only send a handle to another process.
    * The CUDA runtime does not support the fork start method; either the `spawn` or `forkserver` start method are required to use CUDA in subprocesses.







**References**
------------
[1]  https://github.com/snknitin/PyTorch_learn   
[2] https://d2l.ai/chapter_notation/index.html 
[3] https://pytorch.org/docs/stable/notes/multiprocessing.html  