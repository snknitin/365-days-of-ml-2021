# Day 18 of ML 

The **Lottery Ticket Hypothesis** predicts that randomly initialized neural networks contain subnetworks that could be re-trained alone to reach the performance of the full model (Frankle and Carbin 2019). We use two pruning methods to find such subnetworks and to test whether the hypothesis holds: unstructured magnitude pruning and structured pruning.

1) **Magnitude pruning (m-pruning)** where the weights with the lowest magnitude are pruned irrespective of their position in the model. We iteratively prune 10% of the least magnitude weights across the entire fine-tuned model (except the embeddings) and evaluate on dev set, for as long as the performance of the pruned subnetwork is above 90% of the full model. The m-pruned subnetworks are quite similar both across random seeds and across tasks. 
2) **Structured pruning (s-pruning)** of entire components of BERT architecture based on their importance scores: specifically, we 'remove' the least important self-attention heads and MLPs by applying a mask. In each iteration, we prune 10% of BERT heads and 1 MLP, for as long as the performance of the pruned subnetwork is above 90% of the full model. To determine which heads/MLPs to prune, we use a loss-based approximation: the importance scores proposed by Michel, Levy and Neubig (2019) for self-attention heads, which we extend to MLPs. for s-pruning, the “good” subnetworks are quite different across tasks

For both methods the masks are determined with respect to full model performance on a specific dataset. We are interested in finding the subnetworks that enable BERT to perform well on the full set of 9 GLUE tasks (Wang et al. 2018).

The m-pruned subnetworks are quite stable (std mostly around 0.01). But the same cannot be said for s-pruning: there are a few heads are “super-survivors” (i.e. they survive in all random seeds), and some never survive the pruning, but for about 70% of the heads the standard deviation is in the 0.45-0.55 range.
The reason for that appears to be that the importance scores for most BERT heads are equally low. 

We consider three experimental settings:

* the “good” subnetworks: the elements selected from the full model by either s-pruning or m-pruning;
* the “random” subnetworks: the elements randomly sampled from the full model to match the “good” subnetwork size;
* the “bad” subnetworks: the elements that did *not* survive the pruning, plus a few elements sampled from the remaining elements so as to match the good subnetwork size.

The “random” and “bad” subnetworks also generally perform better when re-fine-tuned, but the “bad” subnetworks are consistently worse than “random”.

Overall, it does not seem to be the case that the super-survivor subnetworks consist predominantly of the potentially meaningful self-attention patterns. 

If the success of BERT subnetworks is attributable to the linguistic knowledge they encode, the “super-survivors” should contain considerably more of it. even the self-attention heads that survive the most consistently do not have predominantly the self-attention patterns that are potentially interpretable.


        Our takeaway from this is that s-pruned BERT could be said to have no “losing” tickets.
        This is good news for BERT compression (it's a lottery you can't lose), but bad news for interpretability.
 
 
**References**
------------
[1]  https://thegradient.pub/when-bert-plays-the-lottery-all-tickets-are-winning/  
[2]